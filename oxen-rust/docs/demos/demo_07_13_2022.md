
- ask him his background
- our background
    - how we got to problem
    - watson, fyter
- team
- git for sensory data
    - onboard
    - tooling
    - get data
    - track data
- we're talking to
    - universities
    - friendlies
        - real estate
        - woebot
    - sports teams
- demo
    - clone
    - checkout branches
    - performance
    - training scripts are separate
        - clone, checkout, run train
        - for example here it is in DVC, Git LFS (two bigger competitors)
- asks
    - where does he store data
    - link in tutorials
    - upload any datasets?


# Oxen Experiment Workflow

We have a dataset of videos for basketball action recognition with this distribution of ~2 second videos

`python copy_spacejam.py ~/Datasets/SpaceJam/dataset/ data/BasketballActionRecognition/videos`

`ls data/BasketballActionRecognition/videos/ | cut -d '_' -f 1 | sort | uniq -c | sort -r`

6490 noaction
3490 dribble
1070 pass
 996 block
 426 shoot

`python move_test.py data/BasketballActionRecognition/videos data/BasketballActionRecognition/test 126`

Split out test set of 126 videos from each category. Move these files so that they are never in the train again.

126 noaction
126 dribble
126 block
126 pick
126 shoot

630 videos total to test

Which leaves us these training videos

6364 noaction
3364 dribble
 944 pass
 870 block
 300 shoot

`python split_train.py data/BasketballActionRecognition/videos data/BasketballActionRecognition/train 300`

`ls data/BasketballActionRecognition/train | cut -d '_' -f 1 | sort | uniq -c | sort -r`
 300 shoot
 300 pass
 300 noaction
 300 dribble
 300 block

`ls data/BasketballActionRecognition/test | cut -d '_' -f 1 | sort | uniq -c | sort -r`
 126 shoot
 126 pass
 126 noaction
 126 dribble
 126 block

Init oxen repo, and add videos named with categories

`cd data/BasketballActionRecognition/`

`oxen init .`

`oxen add .`

`oxen commit -m "adding initial videos"`

videos/
    train/
        block_1.mp4
        block_2.mp4
        ....
        shoot_299.mp4
        shoot_300.mp4
    test/
        ....

## Experiment 1

`cd ../../`

`python take_frames_from_video.py data/BasketballActionRecognition/ first`

Start small take first frame of each video

`ls data/BasketballActionRecognition/images/train | cut -d '_' -f 1 | sort | uniq -c | sort -r`

`cd data/BasketballActionRecognition`

`oxen status`

On branch main -> ab28049e028d2a00

Untracked files:
  (use "oxen add <file>..." to update what will be committed)
  images/ with untracked 2133 files

`oxen add images`

`oxen commit -m "adding images"`

`oxen create-remote`

`oxen set-remote origin http://hub.oxen.ai/repositories/BasketballActionRecognition`

*TODO* don't push twice on this command...

`oxen push origin main`

## Experiment 2

`oxen checkout -b experiment/first-middle-last-frame`

`cd ../../`

`python take_frames_from_video.py data/BasketballActionRecognition/ first_mid_last`

`cd data/BasketballActionRecognition`

```
$ oxen status
On branch experiment/first-middle-last-frame -> b627c02771a30b62

Modified files:
  (use "oxen add <file>..." to update what will be committed)
  modified:  images/labels/labels.txt
  modified:  images/annotations/train_annotations.txt
  modified:  images/annotations/test_annotations.txt

Untracked files:
  (use "oxen add <file>..." to update what will be committed)
  images/ with untracked 4260 files
```

`oxen add images`

`oxen commit -m "adding first middle and last frames"`

`oxen checkout main`

Show we are back to orig setup

`ls images/train/ | cut -d '_' -f 1 | sort | uniq -c | sort -r`

Take first, middle, last frame of every video

900 noaction
900 pass
900 block
900 pick
900 shoot

4500 images

## Experiment 3

Checkout another branch to add all the data

`oxen checkout -b experiment/all-frames`

`cd ../../`

`python take_frames_from_video.py data/BasketballActionRecognition/ all`

`oxen status`

`oxen add images`

`oxen commit -m "adding all frames"`

Take all frames of video

5854 block
5919 dribble
6031 noaction
6064 pass
6207 shoot

36000 images


-------------------------

KNOWN BUGS

Cleaning up
- checkout branch
- add & commit new directory of data
- checkout previous commit without that directory
- directory is not removed

Pushing
- Looks like we are pushing data twice when not needed

Failed Push, or corrupted data on server
- If one or more files fail on the push, or the server isn't synced, we should compute hash on server and repush

Checkout Performace
- Seems like we are doing more copies than needed..?

Content Address-able FS
- We are addressing by filename but should have mapping from filename -> content and not copy over multiple times

PERFORMANCE

- SCP
- Weights & Biases
- DVC
- Git LFS

